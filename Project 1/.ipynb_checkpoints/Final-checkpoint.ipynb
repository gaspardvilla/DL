{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "# Import of functions \n",
    "from Functions.get_tests import get_tests\n",
    "from Functions import get\n",
    "from Functions.train_and_test_model import train_and_test_model\n",
    "from Functions.digit_normalization import digit_normalization\n",
    "\n",
    "# Just used here for the final plots in the report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Initialization of all the parameters\n",
    "nb_epochs = 25\n",
    "mini_batch_size = 100\n",
    "nb_rounds = 2\n",
    "\n",
    "# Definition of the train set\n",
    "train_input, train_target, train_classes,_, _, _ \\\n",
    "    = prologue.generate_pair_sets(1000)\n",
    "\n",
    "# Normalization of the train set\n",
    "train_input = digit_normalization(train_input)\n",
    "\n",
    "# Definition of the 100 tests sets\n",
    "Tests = get_tests(nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoWS_NoAL\n",
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n",
      "epoch = 3\n",
      "epoch = 4\n",
      "epoch = 5\n",
      "epoch = 6\n",
      "epoch = 7\n",
      "epoch = 8\n",
      "epoch = 9\n",
      "epoch = 10\n",
      "epoch = 11\n",
      "epoch = 12\n",
      "epoch = 13\n",
      "epoch = 14\n",
      "epoch = 15\n",
      "epoch = 16\n",
      "epoch = 17\n",
      "epoch = 18\n",
      "epoch = 19\n",
      "epoch = 20\n",
      "epoch = 21\n",
      "epoch = 22\n",
      "epoch = 23\n",
      "epoch = 24\n",
      "NoWS_AL\n",
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n",
      "epoch = 3\n"
     ]
    }
   ],
   "source": [
    "# This part is for the plot to compare different architectures for MLP\n",
    "\n",
    "print('NoWS_NoAL')\n",
    "# Test error for NoWS_NoAL\n",
    "NoWS_NoAL = get.mlp_nows_noal(nb_rounds)\n",
    "_, Test_error_NoWS_NoAL, std_deviation_nows_noal \\\n",
    "    = train_and_test_model(NoWS_NoAL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('NoWS_AL')\n",
    "# Test error for NoWS_AL\n",
    "NoWS_AL = get.mlp_nows_al(nb_rounds)\n",
    "_, Test_error_NoWS_AL, std_deviation_nows_al \\\n",
    "    = train_and_test_model(NoWS_AL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('WS_NoAL')\n",
    "# Test error for WS_NoAL\n",
    "WS_NoAL = get.mlp_ws_noal(nb_rounds)\n",
    "_, Test_error_WS_NoAL, std_deviation_ws_noal \\\n",
    "    = train_and_test_model(WS_NoAL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('WS_AL')\n",
    "# Test error for WS_AL\n",
    "WS_AL = get.mlp_ws_al(nb_rounds)\n",
    "_, Test_error_WS_AL, std_deviation_ws_al \\\n",
    "    = train_and_test_model(WS_AL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "# Standard deviation of the tests error\n",
    "print(\"For NoWS_NoAL, after\", nb_epochs, \"epochs, there is\", Test_error_NoWS_NoAL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_nows_noal)\n",
    "print(\"For NoWS_AL, after\", nb_epochs, \"epochs, there is\", Test_error_NoWS_AL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_nows_al)\n",
    "print(\"For WS_NoAL, after\", nb_epochs, \"epochs, there is\", Test_error_WS_NoAL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_ws_noal)\n",
    "print(\"For WS_AL, after\", nb_epochs, \"epochs, there is\", Test_error_WS_AL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_ws_al)\n",
    "    \n",
    "# Plots\n",
    "plt.figure(1)\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, Test_error_NoWS_NoAL, label='Test error for MLP_NoWS_NoAL')\n",
    "plt.plot(epochs, Test_error_NoWS_AL, label='Test error for MLP_NoWS_AL')\n",
    "plt.plot(epochs, Test_error_WS_NoAL, label='Test error for MLP_WS_NoAL')\n",
    "plt.plot(epochs, Test_error_WS_AL, label='Test error for MLP_WS_AL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage of error [%]')\n",
    "plt.title('Test error for different architectures with MLP')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('MLP_vs_architectures.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoWS_NoAL\n",
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n",
      "epoch = 3\n",
      "epoch = 4\n",
      "epoch = 5\n",
      "epoch = 6\n",
      "epoch = 7\n",
      "epoch = 8\n",
      "epoch = 9\n",
      "epoch = 10\n",
      "epoch = 11\n",
      "epoch = 12\n",
      "epoch = 13\n",
      "epoch = 14\n",
      "epoch = 15\n",
      "epoch = 16\n",
      "epoch = 17\n",
      "epoch = 18\n",
      "epoch = 19\n",
      "epoch = 20\n",
      "epoch = 21\n",
      "epoch = 22\n",
      "epoch = 23\n",
      "epoch = 24\n",
      "epoch = 25\n",
      "epoch = 26\n",
      "epoch = 27\n",
      "epoch = 28\n",
      "epoch = 29\n",
      "epoch = 30\n",
      "epoch = 31\n",
      "epoch = 32\n",
      "epoch = 33\n",
      "epoch = 34\n",
      "epoch = 35\n",
      "epoch = 36\n",
      "epoch = 37\n",
      "epoch = 38\n",
      "epoch = 39\n",
      "epoch = 40\n",
      "epoch = 41\n",
      "epoch = 42\n",
      "epoch = 43\n",
      "epoch = 44\n",
      "epoch = 45\n",
      "epoch = 46\n",
      "epoch = 47\n",
      "epoch = 48\n",
      "epoch = 49\n",
      "NoWS_AL\n",
      "epoch = 0\n",
      "epoch = 1\n",
      "epoch = 2\n",
      "epoch = 3\n",
      "epoch = 4\n",
      "epoch = 5\n",
      "epoch = 6\n",
      "epoch = 7\n",
      "epoch = 8\n",
      "epoch = 9\n",
      "epoch = 10\n",
      "epoch = 11\n",
      "epoch = 12\n",
      "epoch = 13\n",
      "epoch = 14\n",
      "epoch = 15\n",
      "epoch = 16\n",
      "epoch = 17\n",
      "epoch = 18\n",
      "epoch = 19\n",
      "epoch = 20\n",
      "epoch = 21\n",
      "epoch = 22\n",
      "epoch = 23\n",
      "epoch = 24\n",
      "epoch = 25\n",
      "epoch = 26\n",
      "epoch = 27\n",
      "epoch = 28\n",
      "epoch = 29\n",
      "epoch = 30\n",
      "epoch = 31\n",
      "epoch = 32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6fb8326a5887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mNoWS_AL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_nows_al\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTest_error_NoWS_AL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_deviation_nows_al\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     = train_and_test_model(NoWS_AL, train_input, train_target, \n\u001b[0m\u001b[1;32m     15\u001b[0m                            train_classes, Tests, nb_epochs, mini_batch_size)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/4ème année/Semestre VIII/Deep Learning/Project/Working folder/Project 1/Functions/train_and_test_model.py\u001b[0m in \u001b[0;36mtrain_and_test_model\u001b[0;34m(Models, train_input, train_target, train_classes, Tests, nb_epochs, mini_batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This part is for the plot to compare different architectures for Conv\n",
    "\n",
    "print('NoWS_NoAL')\n",
    "# Test error for NoWS_NoAL\n",
    "NoWS_NoAL = get.conv_nows_noal(nb_rounds)\n",
    "_, Test_error_NoWS_NoAL, std_deviation_nows_noal \\\n",
    "    = train_and_test_model(NoWS_NoAL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('NoWS_AL')\n",
    "# Test error for NoWS_AL\n",
    "NoWS_AL = get.conv_nows_al(nb_rounds)\n",
    "_, Test_error_NoWS_AL, std_deviation_nows_al \\\n",
    "    = train_and_test_model(NoWS_AL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('WS_NoAL')\n",
    "# Test error for WS_NoAL\n",
    "WS_NoAL = get.conv_ws_noal(nb_rounds)\n",
    "_, Test_error_WS_NoAL, std_deviation_ws_noal \\\n",
    "    = train_and_test_model(WS_NoAL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('WS_AL')\n",
    "# Test error for WS_AL\n",
    "WS_AL = get.conv_ws_al(nb_rounds)\n",
    "_, Test_error_WS_AL, std_deviation_ws_al \\\n",
    "    = train_and_test_model(WS_AL, train_input, train_target, \n",
    "                           train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "# Standard deviation of the tests error\n",
    "print(\"For NoWS_NoAL, after\", nb_epochs, \"epochs, there is\", Test_error_NoWS_NoAL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_nows_noal)\n",
    "print(\"For NoWS_AL, after\", nb_epochs, \"epochs, there is\", Test_error_NoWS_AL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_nows_al)\n",
    "print(\"For WS_NoAL, after\", nb_epochs, \"epochs, there is\", Test_error_WS_NoAL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_ws_noal)\n",
    "print(\"For WS_AL, after\", nb_epochs, \"epochs, there is\", Test_error_WS_AL[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_ws_al)\n",
    "    \n",
    "# Plots\n",
    "plt.figure(2)\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, Test_error_NoWS_NoAL, label='Test error for Conv_NoWS_NoAL')\n",
    "plt.plot(epochs, Test_error_NoWS_AL, label='Test error for Conv_NoWS_AL')\n",
    "plt.plot(epochs, Test_error_WS_NoAL, label='Test error for Conv_WS_NoAL')\n",
    "plt.plot(epochs, Test_error_WS_AL, label='Test error for Conv_WS_AL')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage of error [%]')\n",
    "plt.title('Test error for different architectures with Conv')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Conv_vs_architectures.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is for the plot with MLP_NoWS_NoAL and Conv_NoWS_NoAL architectures\n",
    "\n",
    "print('B')\n",
    "# Train and test error for MLP\n",
    "B = get.conv_ws_al_b(nb_rounds)\n",
    "_, Test_error_B, std_deviation_b \\\n",
    "    = train_and_test_model(B, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('D')\n",
    "# Train and test error for MLP\n",
    "D = get.conv_ws_al_d(nb_rounds)\n",
    "_, Test_error_D, std_deviation_d \\\n",
    "    = train_and_test_model(D, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('BD')\n",
    "# Train and test error for MLP\n",
    "BD = get.conv_ws_al_bd(nb_rounds)\n",
    "_, Test_error_BD, std_deviation_bd \\\n",
    "    = train_and_test_model(BD, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('Noth')\n",
    "# Train and test error for MLP\n",
    "Noth = get.conv_ws_al(nb_rounds)\n",
    "_, Test_error_Noth, std_deviation_noth \\\n",
    "    = train_and_test_model(Noth, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "# Standard deviation of the tests error\n",
    "print(\"For Noth, after\", nb_epochs, \"epochs, there is\", Test_error_Noth[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_noth)\n",
    "print(\"For B, after\", nb_epochs, \"epochs, there is\", Test_error_B[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_b)\n",
    "print(\"For D, after\", nb_epochs, \"epochs, there is\", Test_error_D[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_d)\n",
    "print(\"For BD, after\", nb_epochs, \"epochs, there is\", Test_error_BD[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_bd)\n",
    "\n",
    "# Plots\n",
    "plt.figure(3)\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, Test_error_Noth, label='Test error for Conv_WS_AL')\n",
    "plt.plot(epochs, Test_error_B, label='Test error for Conv_WS_AL_B')\n",
    "plt.plot(epochs, Test_error_D, label='Test error for Conv_WS_AL_D')\n",
    "plt.plot(epochs, Test_error_BD, label='Test error for Conv_WS_AL_BD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage of error [%]')\n",
    "plt.title('Test error for different architectures with Conv')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Conv_B_D_BD.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is for the plot with MLP_NoWS_NoAL and Conv_NoWS_NoAL architectures\n",
    "\n",
    "print('B')\n",
    "# Train and test error for MLP\n",
    "B = get.mlp_ws_al_b(nb_rounds)\n",
    "_, Test_error_B, std_deviation_b \\\n",
    "    = train_and_test_model(B, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('D')\n",
    "# Train and test error for MLP\n",
    "D = get.mlp_ws_al_d(nb_rounds)\n",
    "_, Test_error_D, std_deviation_d \\\n",
    "    = train_and_test_model(D, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('BD')\n",
    "# Train and test error for MLP\n",
    "BD = get.mlp_ws_al_bd(nb_rounds)\n",
    "_, Test_error_BD, std_deviation_bd \\\n",
    "    = train_and_test_model(BD, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('Noth')\n",
    "# Train and test error for MLP\n",
    "Noth = get.mlp_ws_al(nb_rounds)\n",
    "_, Test_error_Noth, std_deviation_noth \\\n",
    "    = train_and_test_model(Noth, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "# Standard deviation of the tests error\n",
    "print(\"For Noth, after\", nb_epochs, \"epochs, there is\", Test_error_Noth[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_noth)\n",
    "print(\"For B, after\", nb_epochs, \"epochs, there is\", Test_error_B[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_b)\n",
    "print(\"For D, after\", nb_epochs, \"epochs, there is\", Test_error_D[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_d)\n",
    "print(\"For BD, after\", nb_epochs, \"epochs, there is\", Test_error_BD[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_bd)\n",
    "\n",
    "# Plots\n",
    "plt.figure(4)\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, Test_error_Noth, label='Test error for MLP_WS_AL')\n",
    "plt.plot(epochs, Test_error_B, label='Test error for MLP_WS_AL_B')\n",
    "plt.plot(epochs, Test_error_D, label='Test error for MLP_WS_AL_D')\n",
    "plt.plot(epochs, Test_error_BD, label='Test error for MLP_WS_AL_BD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage of error [%]')\n",
    "plt.title('Test error for different architectures with MLP')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('MLP_B_D_BD.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part is for the plot to compare the best architecture for MLP and Conv\n",
    "\n",
    "print('MLP')\n",
    "# Train and test error for MLP\n",
    "MLP = get.mlp_ws_al_bd(nb_rounds)\n",
    "Train_error_MLP, Test_error_MLP, std_deviation_mlp \\\n",
    "    = train_and_test_model(MLP, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "print('Conv')\n",
    "# Train and test error for Conv\n",
    "Conv = get.conv_ws_al_bd(nb_rounds)\n",
    "Train_error_Conv, Test_error_Conv, std_deviation_conv \\\n",
    "    = train_and_test_model(Conv, train_input, train_target, \n",
    "                            train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "# Standard deviation of the tests error\n",
    "print(\"For MLP, after\", nb_epochs, \"epochs, there is\", Test_error_MLP[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_mlp)\n",
    "print(\"For Conv, after\", nb_epochs, \"epochs, there is\", Test_error_Conv[-1], \n",
    "      \"% of test error and a standard deviation equal to\", std_deviation_conv)\n",
    "    \n",
    "# Plots\n",
    "plt.figure(5)\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, Train_error_MLP, 'C3--', label='Train error for MLP_WS_AL_BD')\n",
    "plt.plot(epochs, Test_error_MLP, 'C3', label='Test error for MLP_WS_AL_BD')\n",
    "plt.plot(epochs, Train_error_Conv, 'C0--', label='Train error for Conv_WS_AL_BD')\n",
    "plt.plot(epochs, Test_error_Conv, 'C0', label='Test error for Conv_WS_AL_BD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Percentage of error [%]')\n",
    "plt.title('Train and test error for different architectures')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('MLP_vs_Conv_WS_AL_BD.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
