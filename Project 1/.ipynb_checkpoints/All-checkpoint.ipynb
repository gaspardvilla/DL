{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        _, _, result = model(data_input.narrow(0, b, mini_batch_size))\n",
    "        _, predicted_classes = torch.max(result, 1)\n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, train_classes, nb_epochs, mini_batch_size, AL):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    eta = 1e-3\n",
    "    loss_coeff = 10\n",
    "    optimizer = optim.Adam(model.parameters(), lr = eta)\n",
    "    \n",
    "    for e in range(nb_epochs):    \n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            digit1, digit2, result = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                \n",
    "            loss_result = criterion(result, train_target.narrow(0, b, mini_batch_size))\n",
    "            loss_digit1 = criterion(digit1, train_classes[:,0].narrow(0, b, mini_batch_size))\n",
    "            loss_digit2 = criterion(digit2, train_classes[:,1].narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            if AL:\n",
    "                loss = loss_result + loss_coeff*loss_digit1 + loss_coeff*loss_digit2\n",
    "            else:\n",
    "                loss = loss_result\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_NoWS_NoAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_NoWS_NoAL, self).__init__()\n",
    "        \n",
    "        nb_hidden = 100\n",
    "        input_size = 14*14\n",
    "        \n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1) #Technically this is already in the nn.CrossEntropyLoss()\n",
    "        )\n",
    "        \n",
    "        self.layers2 = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.layers_comp = nn.Sequential(\n",
    "            nn.Linear(20, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "        \n",
    "        first_digit = first_digit.view(first_digit.size(0),-1) #torch.reshape() can also be used\n",
    "        second_digit = second_digit.view(second_digit.size(0),-1)\n",
    "        \n",
    "        first_digit = self.layers1(first_digit)\n",
    "        second_digit = self.layers2(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layers_comp(result)\n",
    "    \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_WS_NoAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_WS_NoAL, self).__init__()\n",
    "        \n",
    "        nb_hidden = 100\n",
    "        input_size = 14*14\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1) #Technically this is already in the nn.CrossEntropyLoss()\n",
    "        )\n",
    "        \n",
    "        self.layers_comp = nn.Sequential(\n",
    "            nn.Linear(20, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "        \n",
    "        first_digit = first_digit.view(first_digit.size(0),-1) #torch.reshape() can also be used\n",
    "        second_digit = second_digit.view(second_digit.size(0),-1)\n",
    "        \n",
    "        first_digit = self.layers(first_digit)\n",
    "        second_digit = self.layers(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layers_comp(result)\n",
    "    \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_NoWS_AL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_NoWS_AL, self).__init__()\n",
    "        \n",
    "        nb_hidden = 100\n",
    "        input_size = 14*14\n",
    "        \n",
    "        self.layers1 = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1) #Technically this is already in the nn.CrossEntropyLoss()\n",
    "        )\n",
    "        \n",
    "        self.layers2 = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.layers_comp = nn.Sequential(\n",
    "            nn.Linear(20, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "        \n",
    "        first_digit = first_digit.view(first_digit.size(0),-1) #torch.reshape() can also be used\n",
    "        second_digit = second_digit.view(second_digit.size(0),-1)\n",
    "        \n",
    "        first_digit = self.layers1(first_digit)\n",
    "        second_digit = self.layers2(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layers_comp(result)\n",
    "    \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_WS_AL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_WS_AL, self).__init__()\n",
    "        \n",
    "        nb_hidden = 100\n",
    "        input_size = 14*14\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, nb_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.Linear(nb_hidden, nb_hidden),\n",
    "            nn.Linear(nb_hidden, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.layers_comp = nn.Sequential(\n",
    "            nn.Linear(20, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2000, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "        \n",
    "        first_digit = first_digit.view(first_digit.size(0),-1) #torch.reshape() can also be used\n",
    "        second_digit = second_digit.view(second_digit.size(0),-1)\n",
    "        \n",
    "        first_digit = self.layers(first_digit)\n",
    "        second_digit = self.layers(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layers_comp(result)\n",
    "    \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_NoWS_NoAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_NoWS_NoAL, self).__init__()\n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1_first_digit = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1_second_digit = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2_first_digit = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2_second_digit = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Formula to get out_put size (in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "        # first layer (14-5+2*2)/1 +1 = 14/2 = 7\n",
    "        # second layer (7 -4 +2*2)/1 +1 = 8/2 = 4\n",
    "        # 4 * 4 * 64 input features, 1000 output features\n",
    "        self.fc1_first_digit = nn.Linear(4 * 4 * 64, 1000)\n",
    "        self.fc1_second_digit = nn.Linear(4 * 4 * 64, 1000)\n",
    "        \n",
    "        # 1000 input features, 2 output features\n",
    "        self.fc2_first_digit = nn.Linear(1000, 10)\n",
    "        self.fc2_second_digit = nn.Linear(1000, 10)\n",
    "        \n",
    "        #Comparison of the two digits\n",
    "        self.layer_comp = nn.Sequential(\n",
    "            nn.Linear(20, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "\n",
    "        first_digit = self.layer1_first_digit(first_digit)\n",
    "        second_digit = self.layer1_second_digit(second_digit)\n",
    "        \n",
    "        first_digit = self.layer2_first_digit(first_digit)\n",
    "        second_digit = self.layer2_second_digit(second_digit)\n",
    "    \n",
    "        first_digit = F.relu(self.fc1_first_digit(first_digit.view(-1, 4 * 4 * 64)))\n",
    "        second_digit = F.relu(self.fc1_second_digit(second_digit.view(-1, 4 * 4 * 64)))\n",
    "        \n",
    "        first_digit = self.fc2_first_digit(first_digit)\n",
    "        second_digit = self.fc2_second_digit(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layer_comp(result)\n",
    "        \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_WS_NoAL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_WS_NoAL, self).__init__()\n",
    "        \n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "\n",
    "        \n",
    "        # Formula to get out_put size (in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "        # first layer (14-5+2*2)/1 +1 = 14/2 = 7\n",
    "        # second layer (7 -4 +2*2)/1 +1 = 8/2 = 4\n",
    "        # 4 * 4 * 64 input features, 1000 output features\n",
    "        self.fc1 = nn.Linear(4 * 4 * 64, 1000)\n",
    "        \n",
    "        # 1000 input features, 2 output features\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "        #Comparison of the two digits\n",
    "        self.layer_comp = nn.Sequential(\n",
    "            nn.Linear(20, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "\n",
    "        first_digit = self.layer1(first_digit)\n",
    "        second_digit = self.layer1(second_digit)\n",
    "        \n",
    "        first_digit = self.layer2(first_digit)\n",
    "        second_digit = self.layer2(second_digit)\n",
    "    \n",
    "        first_digit = F.relu(self.fc1(first_digit.view(-1, 4 * 4 * 64)))\n",
    "        second_digit = F.relu(self.fc1(second_digit.view(-1, 4 * 4 * 64)))\n",
    "        \n",
    "        first_digit = self.fc2(first_digit)\n",
    "        second_digit = self.fc2(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layer_comp(result)\n",
    "        \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_NoWS_AL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_NoWS_AL, self).__init__()\n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1_first_digit = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1_second_digit = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2_first_digit = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2_second_digit = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Formula to get out_put size (in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "        # first layer (14-5+2*2)/1 +1 = 14/2 = 7\n",
    "        # second layer (7 -4 +2*2)/1 +1 = 8/2 = 4\n",
    "        # 4 * 4 * 64 input features, 1000 output features\n",
    "        self.fc1_first_digit = nn.Linear(4 * 4 * 64, 1000)\n",
    "        self.fc1_second_digit = nn.Linear(4 * 4 * 64, 1000)\n",
    "        \n",
    "        # 1000 input features, 2 output features\n",
    "        self.fc2_first_digit = nn.Linear(1000, 10)\n",
    "        self.fc2_second_digit = nn.Linear(1000, 10)\n",
    "        \n",
    "        #Comparison of the two digits\n",
    "        self.layer_comp = nn.Sequential(\n",
    "            nn.Linear(20, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "\n",
    "        first_digit = self.layer1_first_digit(first_digit)\n",
    "        second_digit = self.layer1_second_digit(second_digit)\n",
    "        \n",
    "        first_digit = self.layer2_first_digit(first_digit)\n",
    "        second_digit = self.layer2_second_digit(second_digit)\n",
    "    \n",
    "        first_digit = F.relu(self.fc1_first_digit(first_digit.view(-1, 4 * 4 * 64)))\n",
    "        second_digit = F.relu(self.fc1_second_digit(second_digit.view(-1, 4 * 4 * 64)))\n",
    "        \n",
    "        first_digit = self.fc2_first_digit(first_digit)\n",
    "        second_digit = self.fc2_second_digit(second_digit)\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layer_comp(result)\n",
    "        \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_WS_AL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_WS_AL, self).__init__()\n",
    "        \n",
    "        #Input channels = 1, output channels = 32\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        #Input channels = 32, output channels = 64\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2))\n",
    "        \n",
    "        # Formula to get out_put size (in_size - kernel_size + 2*(padding)) / stride) + 1\n",
    "        # first layer (14 - 3 + 2*1) + 1 = 14/2 = 7\n",
    "        # second layer (7 - 2 + 2*1) + 1 = 8/2 = 4\n",
    "        # 4 * 4 * 64 input features, 1000 output features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4 * 4 * 64, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 10))\n",
    "\n",
    "        #Comparison of the two digits\n",
    "        self.layer_comp = nn.Sequential(\n",
    "            nn.Linear(20, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        first_digit = x[:,[0]]\n",
    "        second_digit = x[:,[1]]\n",
    "\n",
    "        first_digit = self.layer1(first_digit)\n",
    "        second_digit = self.layer1(second_digit)\n",
    "        \n",
    "        first_digit = self.layer2(first_digit)\n",
    "        second_digit = self.layer2(second_digit)\n",
    "    \n",
    "        first_digit = self.fc(first_digit.view(-1, 4 * 4 * 64))\n",
    "        second_digit = self.fc(second_digit.view(-1, 4 * 4 * 64))\n",
    "        \n",
    "        result = torch.cat((first_digit, second_digit), dim=1, out=None)\n",
    "        result = self.layer_comp(result)\n",
    "        \n",
    "        return first_digit, second_digit, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tests(n):\n",
    "    M = []\n",
    "    for k in range (0, n):\n",
    "        L = []\n",
    "        _, _, _, test_input, test_target, test_classes =  prologue.generate_pair_sets(1000)\n",
    "        L.append(test_input)\n",
    "        L.append(test_target)\n",
    "        L.append(test_classes)\n",
    "        M.append(L)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_yolo(model, nb_epochs, AL):\n",
    "    \n",
    "    # model = Conv_WS_AL()\n",
    "    mini_batch_size = 100\n",
    "    Train_error = []\n",
    "    Test_error = []\n",
    "    \n",
    "    train_input, train_target, train_classes,_, _, _ \\\n",
    "    = prologue.generate_pair_sets(1000)\n",
    "\n",
    "    for e in range(1, nb_epochs+1):\n",
    "        \n",
    "        # AL = True\n",
    "        train_model(model, train_input, train_target, train_classes, e, mini_batch_size, AL)\n",
    "        L = get_tests(10)\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target, mini_batch_size)\n",
    "        Train_error.append(nb_train_errors/10)\n",
    "\n",
    "        avg_nb_test_error = 0\n",
    "\n",
    "        for k in range (0, len(L)):\n",
    "            nb_test_errors = compute_nb_errors(model, L[k][0], L[k][1], mini_batch_size)\n",
    "            avg_nb_test_error += nb_test_errors\n",
    "\n",
    "        avg_nb_test_error /= len(L)\n",
    "        Test_error.append(avg_nb_test_error/10)\n",
    "        \n",
    "    return Train_error, Test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train_error, Test_error = main_yolo(3)\n",
    "#print(Train_error)\n",
    "#print(Test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:58: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:63: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/churchhyll/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:53: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8b5871f160>,\n",
       " <matplotlib.lines.Line2D at 0x7f8b5871f190>,\n",
       " <matplotlib.lines.Line2D at 0x7f8b5871f1c0>,\n",
       " <matplotlib.lines.Line2D at 0x7f8b5871f310>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_1 = Conv_WS_AL()\n",
    "model_2 = MLP_WS_AL()\n",
    "\n",
    "AL = True\n",
    "NoAL = False\n",
    "\n",
    "nb_epochs = 25\n",
    "\n",
    "train_error_1, test_error_1 = main_yolo(model_1, nb_epochs, AL)\n",
    "train_error_2, test_error_2 = main_yolo(model_2, nb_epochs, AL)\n",
    "\n",
    "# yolo\n",
    "epochs = torch.linspace(1, nb_epochs, steps=nb_epochs)\n",
    "plt.plot(epochs, train_error_1,'r--', epochs, test_error_1,'r',\n",
    "         epochs, train_error_2, 'b--', epochs, test_error_2, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
