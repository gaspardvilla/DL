{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "from Supplementary.Modules import *\n",
    "from Supplementary.Functions import *\n",
    "\n",
    "# Just use for the plots at the end\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import imshow, colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Initialization of our principal parameters\n",
    "nb_epochs = 100\n",
    "mini_batch_size = 10\n",
    "\n",
    "# This gives the nb of models that we will train and test to have conclusive\n",
    "# results.\n",
    "nb_rounds = 10\n",
    "\n",
    "# Initialization of training set\n",
    "train_input, train_classes, _, _ = create_problem(1000)\n",
    "\n",
    "# initialization of our test sets\n",
    "Tests = get_tests(nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of all the models with MSE loss\n",
    "Model_ReLU_MSE, Model_Tanh_MSE, Model_Sigmoid_MSE, Model_Leaky_ReLU_MSE, Model_ELU_MSE, _, _ = \\\n",
    "                                get_Models(LossMSE(), nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as ReLU, with MSE loss\n",
    "Train_error_ReLU_MSE, Test_error_ReLU_MSE, std_deviation_ReLU_MSE = \\\n",
    "    train_and_test_model(Model_ReLU_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with ReLU and LossMSE \n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with ReLU and LossMSE is\",Train_error_ReLU_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with ReLU and LossMSE \n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with ReLU and LossMSE is\",Test_error_ReLU_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_ReLU_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as Tanh, with MSE loss\n",
    "Train_error_Tanh_MSE, Test_error_Tanh_MSE, std_deviation_Tanh_MSE = \\\n",
    "    train_and_test_model(Model_Tanh_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Tanh and LossMSE\n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with Tanh and LossMSE is\",Train_error_Tanh_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Tanh and LossMSE \n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with Tanh and LossMSE is\",Test_error_Tanh_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Tanh_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as a Sigmoid, with MSE loss\n",
    "Train_error_Sigmoid_MSE, Test_error_Sigmoid_MSE, std_deviation_Sigmoid_MSE = \\\n",
    "    train_and_test_model(Model_Sigmoid_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Sigmoid and LossMSE \n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with Sigmoid and LossMSE is\",Train_error_Sigmoid_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Sigmoid and LossMSE \n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with Sigmoid and LossMSE is\",Test_error_Sigmoid_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Sigmoid_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as a Leaky ReLU, with MSE loss\n",
    "Train_error_Leaky_ReLU_MSE, Test_error_Leaky_ReLU_MSE, std_deviation_Leaky_ReLU_MSE = \\\n",
    "    train_and_test_model(Model_Leaky_ReLU_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Leaky_ReLU and LossMSE \n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with Leaky_ReLU and LossMSE is\",Train_error_Leaky_ReLU_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Leaky_ReLU and LossMSE \n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with Leaky_ReLU and LossMSE is\",Test_error_Leaky_ReLU_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Leaky_ReLU_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as ELU, with MSE loss\n",
    "Train_error_ELU_MSE, Test_error_ELU_MSE, std_deviation_ELU_MSE = \\\n",
    "    train_and_test_model(Model_ELU_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with ELU and LossMSE \n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with ELU and LossMSE is\",Train_error_ELU_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with ELU and LossMSE \n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with ELU and LossMSE is\",Test_error_ELU_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_ELU_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of all the models with Cross Entropy (CE) loss and dropout models\n",
    "_, Model_Tanh_CE, _, _, _, Model_Tanh_Dropout_MSE, Model_Tanh_Dropout_CE = \\\n",
    "                                get_Models(CrossEntropyLoss(), nb_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as Tanh, with Cross Entropy (CE) loss\n",
    "Train_error_Tanh_CE, Test_error_Tanh_CE, std_deviation_Tanh_CE = \\\n",
    "    train_and_test_model(Model_Tanh_CE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average train_error on\", nb_rounds,\"Sequential models with Tanh and Cross-Entropy Loss is\",Train_error_Tanh_CE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average test_error on\", nb_rounds,\"Sequential models with Tanh and Cross-Entropy Loss is\",Test_error_Tanh_CE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Tanh_CE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as Tanh, with MSE loss and dropout\n",
    "Train_error_Tanh_Dropout_MSE, Test_error_Tanh_Dropout_MSE, std_deviation_Tanh_Dropout_MSE = \\\n",
    "    train_and_test_model(Model_Tanh_Dropout_MSE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average train_error on\", nb_rounds,\"Tanh with Dropout and MSE Loss is\",Train_error_Tanh_Dropout_MSE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average test_error on\", nb_rounds,\"Tanh with Dropout and MSE Loss is\",Test_error_Tanh_Dropout_MSE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Tanh_Dropout_MSE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function as Tanh, with Cross Entropy (CE) loss and dropout\n",
    "Train_error_Tanh_Dropout_CE, Test_error_Tanh_Dropout_CE, std_deviation_Tanh_Dropout_CE = \\\n",
    "    train_and_test_model(Model_Tanh_Dropout_CE, train_input, train_classes, Tests, nb_epochs, mini_batch_size)\n",
    "\n",
    "#Average train_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average train_error on\", nb_rounds,\"Tanh with Dropout and Cross-Entropy Loss is\",Train_error_Tanh_Dropout_CE[0],'%')\n",
    "\n",
    "#Average test_error (%) on number_of_rounds Sequential models with Tanh and Cross-Entropy Loss\n",
    "print(\"Average test_error on\", nb_rounds,\"Tanh with Dropout and Cross-Entropy Loss is\",Test_error_Tanh_Dropout_CE[0],\"%\")\n",
    "\n",
    "#Standard deviation corresponding to the average on test_error just above\n",
    "print(\"Standard deviation corresponding to the average on test_error just above is \", std_deviation_Tanh_Dropout_CE, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of our problem\n",
    "train_input, train_classes, _, _ = create_problem(1000)\n",
    "nb_epochs = 100\n",
    "mini_batch_size = 1\n",
    "\n",
    "# Initialization of the model with Cross Entropy loss\n",
    "model = Sequential([Linear(2,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,2), Leaky_ReLU()], CrossEntropyLoss())\n",
    "model.lr_method(\"Adam\", 1.0e-3)\n",
    "train_model(model, train_input, train_classes, nb_epochs, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the grid for the plots\n",
    "x = torch.linspace(0.0, 1.0, 1000)\n",
    "y = torch.linspace(0.0, 1.0, 1000)\n",
    "n = x.size(0)\n",
    "grid_x, grid_y = torch.meshgrid(x,y)\n",
    "Z = torch.empty(grid_x.size())\n",
    "\n",
    "# Get the \"probability\" that each point is in the circle, given by the output \n",
    "# of the model going through the soft_max function.\n",
    "for k in range(grid_x.size(0)):\n",
    "    for l in range(0, n, mini_batch_size):        \n",
    "        xx = grid_x[k][l:l+mini_batch_size].t().resize_((mini_batch_size,1))\n",
    "        yy = grid_y[k][l:l+mini_batch_size].t().resize_((mini_batch_size,1))\n",
    "        e = torch.cat([xx, yy], 1)\n",
    "        \n",
    "        if model.loss.is_MSE():\n",
    "            # MSE Loss\n",
    "            Z[k][l:l+mini_batch_size] = model.forward(e).resize(mini_batch_size)\n",
    "        else:\n",
    "            # Cross Entropy loss\n",
    "            output = model.forward(e)\n",
    "            output = soft_max(output)\n",
    "            \n",
    "            # We take the second colum because we want the \"probability\" that \n",
    "            # the point is in the circle. It is given in the second column.\n",
    "            second_column = torch.tensor([1])\n",
    "            Z[k][l:l+mini_batch_size] = output.index_select(1,second_column).resize(mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First figure\n",
    "plt.figure(1)\n",
    "figure, axes = plt.subplots()\n",
    "color_map = plt.cm.get_cmap('RdYlBu')\n",
    "reversed_color_map = color_map.reversed()\n",
    "\n",
    "# Plot of the tensor\n",
    "im = imshow(Z, origin='lower', extent=[0,1,0,1], cmap=reversed_color_map)\n",
    "\n",
    "# Plot of the true circle of radius R\n",
    "axes = plt.gca()\n",
    "R = 1 / math.sqrt(2 * math.pi)\n",
    "Drawing_uncolored_circle = plt.Circle((0.5, 0.5 ), R, fill=False, linestyle = '--', label='Target circle')\n",
    "axes.set_aspect(1)\n",
    "axes.add_artist(Drawing_uncolored_circle)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Plot of the results given by the trained model')\n",
    "plt.legend()\n",
    "plt.legend(handles=[Drawing_uncolored_circle], loc='upper right')\n",
    "\n",
    "colorbar(im)\n",
    "plt.savefig('Final_results.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second figure\n",
    "plt.figure(2)\n",
    "Label = train_classes.view(-1).float()\n",
    "x1 = (train_input.narrow(1,0,1).view(-1) * Label);\n",
    "y1 = (train_input.narrow(1,1,1).view(-1) * Label);\n",
    "x0 = (train_input.narrow(1,0,1).view(-1) * (1-Label));\n",
    "y0 = (train_input.narrow(1,1,1).view(-1) * (1-Label));\n",
    "plt.figure(num=None, figsize=(8, 4), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "axes = plt.gca();\n",
    "Lab0, = plt.plot(x1, y1, 'C3o', label='Label 0');\n",
    "Lab1, = plt.plot(x0, y0, 'C0o', label='Label 1');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "# Plot of the true circle of radius R\n",
    "R = 1 / math.sqrt(2 * math.pi)\n",
    "Drawing_uncolored_circle = plt.Circle((0.5, 0.5 ), R, fill=False, linestyle = '--', label='Target circle')\n",
    "axes.set_aspect(1)\n",
    "axes.add_artist(Drawing_uncolored_circle)\n",
    "\n",
    "plt.title('Plot of train set and the target cicle')\n",
    "plt.legend()\n",
    "plt.legend(handles=[Lab1, Lab0, Drawing_uncolored_circle], loc='upper right')\n",
    "axes.set_xlim([0,1]);\n",
    "axes.set_ylim([0,1]);\n",
    "plt.savefig('Train_set.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
