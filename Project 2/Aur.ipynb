{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x21862185088>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Just use for the plots at the end\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import exp,arange\n",
    "from pylab import meshgrid,cm,imshow,contour,clabel,colorbar,axis,title,show\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self , *input):\n",
    "        raise  NotImplementedError\n",
    "        \n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise  NotImplementedError\n",
    "        \n",
    "    def param(self): # These are the layers of the network\n",
    "        return  []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dropout = False\n",
    "        self.linear = False\n",
    "        \n",
    "    def is_dropout(self):\n",
    "        self.dropout = True\n",
    "    \n",
    "    def is_linear(self):\n",
    "        self.linear = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, param, Loss):\n",
    "        super().__init__()\n",
    "        self.model = (param)\n",
    "        self.loss = Loss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, output, target):\n",
    "        grad = self.loss.backward(target, output)\n",
    "        \n",
    "        for layer in reversed(self.model):\n",
    "            grad = layer.backward(grad)\n",
    "        \n",
    "        Loss = self.loss.forward(target, output)\n",
    "        return Loss\n",
    "    \n",
    "    def Train(self):\n",
    "        for layer in self.model:\n",
    "            if layer.dropout:\n",
    "                layer.Train()\n",
    "        \n",
    "    def Eval(self):\n",
    "        for layer in self.model:\n",
    "            if layer.dropout:\n",
    "                layer.Eval()\n",
    "    \n",
    "    def lr_method(self, method, lr):\n",
    "        for layer in self.model:\n",
    "            if layer.linear:\n",
    "                layer.change_lr_method(method, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, in_, out_):\n",
    "        super().__init__()\n",
    "        self.in_ = in_\n",
    "        self.out_ = out_\n",
    "        self.is_linear()\n",
    "        self.lr = 0.005\n",
    "        self.lr_method = 'constant'\n",
    "        \n",
    "        # Capture the term at each layer before the passage in the layer\n",
    "        # and the activation function.\n",
    "        self.x = torch.zeros(out_)\n",
    "        \n",
    "        # Initialization of Adam for weight and bias\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.eps = 1.0e-8\n",
    "        self.eta = 1.0e-1\n",
    "        self.mw = torch.zeros(out_)\n",
    "        self.mb = torch.zeros(out_)\n",
    "        self.vw = 0.0\n",
    "        self.vb = 0.0\n",
    "        \n",
    "        # Initialization of the weights and the bias\n",
    "        param = 1. / math.sqrt(in_)\n",
    "        self.weight = torch.empty(self.in_, self.out_).uniform_(-param, param)\n",
    "        self.bias = torch.empty(self.out_).uniform_(-param, param)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x.mm(self.weight) + self.bias\n",
    "    \n",
    "    def set_Lr(self, lr):\n",
    "        self.lr = lr\n",
    "        return\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \n",
    "        if self.lr_method == \"Adam\":\n",
    "            \n",
    "            # Adam method for the learning rate\n",
    "            gw = self.x.t().mm(grad)\n",
    "            self.mw = ((self.beta1 * self.mw) + ((1 - self.beta1) * gw))\n",
    "            mh = (1 / (1 - self.beta1)) * self.mw\n",
    "            self.vw = ((self.beta2 * self.vw) + ((1 - self.beta2) * (gw.norm()**2)))\n",
    "            vh = (1 / (1 - self.beta2)) * self.vw\n",
    "            self.weight = self.weight - ((self.eta / (vh.sqrt() + self.eps)) * mh)\n",
    "\n",
    "            self.mb = ((self.beta1 * self.mb) + ((1 - self.beta1) * grad))\n",
    "            mh = (1 / (1 - self.beta1)) * self.mb\n",
    "            self.vb = ((self.beta2 * self.vb) + ((1 - self.beta2) * (grad.norm()**2)))\n",
    "            vh = (1 / (1 - self.beta2)) * self.vb\n",
    "            self.bias = self.bias - ((self.eta / (vh.sqrt() + self.eps)) * mh)\n",
    "            grad = grad.mm(self.weight.t())\n",
    "            \n",
    "        elif self.lr_method == \"constant\":\n",
    "            \n",
    "            # Constant learning rate\n",
    "            self.weight = self.weight - self.lr * self.x.t().mm(grad)\n",
    "            self.bias = self.bias - self.lr * grad * 1\n",
    "            grad = grad.mm(self.weight.t())\n",
    "            \n",
    "        return grad\n",
    "    \n",
    "    def weight(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def bias(self):\n",
    "        return self.bias\n",
    "    \n",
    "    def change_lr_method(self, method, lr):\n",
    "        self.lr = lr\n",
    "        self.lr_method = method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Layer) :\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.p = 0.\n",
    "        self.is_dropout()\n",
    "        self.train = True\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        n = torch.ones(x.size())\n",
    "        if self.train:\n",
    "            n = torch.bernoulli(n) * (1 - self.p)\n",
    "        return x * n\n",
    "        \n",
    "    def backward(self, x):\n",
    "        return x\n",
    "    \n",
    "    def Train(self):\n",
    "        self.train = True\n",
    "        \n",
    "    def Eval(self):\n",
    "        self.train = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "    \n",
    "    def forward(self, data_target, data_output):\n",
    "        loss = (data_output - data_target).pow(2).sum()\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, data_target, data_output):\n",
    "        dloss = 2 * (data_output - data_target)\n",
    "        return dloss\n",
    "    \n",
    "    def is_MSE(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, data_target, data_output):\n",
    "        output = data_output.to(dtype=torch.float)\n",
    "        target = data_target.resize_(data_target.size(0), 1)\n",
    "        \n",
    "        zer = torch.zeros(target.size()).int()\n",
    "        target = torch.cat((target,zer), 1)\n",
    "    \n",
    "        first_column = torch.tensor([0])\n",
    "        loss = output.gather(1,target).index_select(1,first_column).exp()\n",
    "        \n",
    "        # To avoid numerical error in the computation\n",
    "        maxx = loss.max()\n",
    "        \n",
    "        loss = (loss * maxx) / (output.exp().sum(1) * maxx)\n",
    "        loss = -(loss.log().mean())\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, data_target, data_output):\n",
    "        # New version\n",
    "        N = data_target.size(0)\n",
    "        dloss = data_output.exp()\n",
    "        dloss = dloss / dloss.sum(1).resize_(N,1)\n",
    "        \n",
    "        add = data_target-1\n",
    "        add = torch.cat((add, -data_target), 1)\n",
    "        dloss = (1/N) * (dloss + add)\n",
    "        return dloss\n",
    "    \n",
    "    def is_MSE(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    \n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        self.save = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x.clamp(min = 0)\n",
    "        self.save = x\n",
    "        return y\n",
    "    \n",
    "    def backward(self, x):\n",
    "        y = self.save > 0\n",
    "        return y.float() * x\n",
    "         \n",
    "    def print(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leaky_ReLU(Layer):\n",
    "    \n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        self.s = 0\n",
    "        self.alpha = 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = torch.max(self.alpha * x, x)\n",
    "        self.s = x\n",
    "        return y\n",
    "    \n",
    "    def backward(self, x):\n",
    "        y = ((self.s > 0) * (1 - self.alpha)) + self.alpha\n",
    "        return y.float() * x\n",
    "         \n",
    "    def print(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELU(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = 0\n",
    "        self.alpha = 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = ((x > 0).float() * x) + (0 >= x) * self.alpha * (torch.exp(x) - 1)\n",
    "        self.s = x\n",
    "        return y\n",
    "    \n",
    "    def backward(self, x):\n",
    "        y = ((self.s > 0) * (1 - self.alpha * torch.exp(self.s))) + self.alpha * torch.exp(self.s)\n",
    "        return y.float() * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Layer) :\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.save = 0\n",
    "    \n",
    "    def  forward(self, x):\n",
    "        self.save = x\n",
    "        return torch.div(x.exp() - (-x).exp(), x.exp() + (-x).exp())\n",
    "        \n",
    "    def  backward(self, x):\n",
    "        return (1 - torch.div(self.save.exp() - \n",
    "                    (-self.save).exp(), self.save.exp() + (-self.save).exp())**2) * x\n",
    "        \n",
    "    def print(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.s = 0\n",
    "        self.lbd = 3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + torch.exp(-self.lbd * x))\n",
    "        self.s = x\n",
    "        return y\n",
    "    \n",
    "    def backward(self, x):\n",
    "        y = self.lbd * torch.exp(-self.s) / ((1 + torch.exp(-self.lbd * self.s))**2)\n",
    "        return y.float() * x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_batch(input_size, mini_batch_size):\n",
    "    \n",
    "    # This function return a 2D tensor that is the rando selection of inputs for our\n",
    "    # stochastic gradient method, taking in count the number of mini_batches.\n",
    "    \n",
    "    # We suppose here that our mini_batch_size is well chosen taking in count the fact\n",
    "    # that it divides input_size.\n",
    "    \n",
    "    # Initialization\n",
    "    L = int(input_size / mini_batch_size)\n",
    "    new_batch = torch.ones(L, mini_batch_size)\n",
    "    \n",
    "    indices = torch.randperm(input_size)\n",
    "    for k in range(L):\n",
    "        new_batch[k] = indices[k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_classes, nb_epochs, mini_batch_size):\n",
    "    \n",
    "    h_step = 1e-3\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        random_batches = create_random_batch(train_input.size(0), mini_batch_size).tolist()\n",
    "        for batch in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, batch, mini_batch_size))\n",
    "            loss = model.backward(output, train_classes.narrow(0, batch, mini_batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, data_input, data_target, mini_batch_size):\n",
    "    \n",
    "    nb_data_errors = 0\n",
    "\n",
    "    for b in range(0, data_input.size(0), mini_batch_size):\n",
    "        result = model.forward(data_input.narrow(0, b, mini_batch_size))\n",
    "        \n",
    "        if model.loss.is_MSE():\n",
    "            # If the loss function is MSE\n",
    "            predicted_classes = (result >= 0.5).int()\n",
    "        else:\n",
    "            # If the loss function is CrossEntropy\n",
    "            _, predicted_classes = torch.max(result, 1)\n",
    "        \n",
    "        for k in range(mini_batch_size):\n",
    "            if data_target[b + k] != predicted_classes[k]:\n",
    "                nb_data_errors = nb_data_errors + 1\n",
    "                \n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_problem(nb_samples):\n",
    "    \n",
    "    # Remark: the function .uniform return a uniform distribution on [0,1) instead of [0,1],\n",
    "    # but in our case it's not a problem since it is only a train and a test set on a circle\n",
    "    # that do not touch the border of the set [0,1]^2.\n",
    "    train_input = torch.empty(nb_samples, 2).uniform_(0, 1)\n",
    "    test_input = torch.empty(nb_samples, 2).uniform_(0, 1)\n",
    "    \n",
    "    # Radius of our circle\n",
    "    R = 1 / math.sqrt(2 * math.pi)\n",
    "    \n",
    "    train_classes = train_input.sub(0.5).pow(2).sum(1).sub(R**2).sign().sub(1).div(-2).long().resize_((nb_samples,1))\n",
    "    test_classes = test_input.sub(0.5).pow(2).sum(1).sub(R**2).sign().sub(1).div(-2).long().resize_((nb_samples,1))\n",
    "    \n",
    "    return train_input, train_classes, test_input, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tests(n):\n",
    "    M = []\n",
    "    for k in range (n):\n",
    "        L = []\n",
    "        _, _, test_input, test_classes =  create_problem(1000)\n",
    "        L.append(test_input)\n",
    "        L.append(test_classes)\n",
    "        M.append(L)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ReLU = Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,1), ReLU()], LossMSE())\n",
    "model_Tanh = Sequential([Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,25), Tanh(), Linear(25,1), Tanh()], LossMSE())\n",
    "model_Sigmoid = Sequential([Linear(2,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,1), Sigmoid()], LossMSE())\n",
    "model_Leaky_ReLU = Sequential([Linear(2,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,1), Leaky_ReLU()], LossMSE())\n",
    "model_ELU = Sequential([Linear(2,25), ELU(), Linear(25,25), ELU(), Linear(25,25), ELU(), Linear(25,1), ELU()], LossMSE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input_list = []\n",
    "train_classes_list = []\n",
    "test_input_list = []\n",
    "test_classes_list = []\n",
    "\n",
    "for i in range (0, 20):\n",
    "    train_input, train_classes, test_input, test_classes = create_problem(1000)\n",
    "    train_input_list.append(train_input)\n",
    "    train_classes_list.append(train_classes)\n",
    "    test_input_list.append(test_input)\n",
    "    test_classes_list.append(test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_ReLU_List = []\n",
    "Model_Tanh_List = []\n",
    "Model_Sigmoid_List = []\n",
    "Model_Leaky_ReLU_List = []\n",
    "Model_ELU_List = []\n",
    "Loss = LossMSE()\n",
    "\n",
    "for k in range (0, 20):\n",
    "    Model_ReLU_List.append(Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,1), ReLU()], LossMSE()))\n",
    "    Model_Tanh_List.append(Sequential([Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,25), Tanh(), Linear(25,1), Tanh()], LossMSE()))\n",
    "    Model_Sigmoid_List.append(Sequential([Linear(2,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,1), Sigmoid()], LossMSE()))\n",
    "    Model_Leaky_ReLU_List.append(Sequential([Linear(2,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,1), Leaky_ReLU()], LossMSE()))\n",
    "    Model_ELU_List.append(Sequential([Linear(2,25), ELU(), Linear(25,25), ELU(), Linear(25,25), ELU(), Linear(25,1), ELU()], LossMSE()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_model(Model, train_input_list, train_classes_list, test_input_list, test_classes_list):\n",
    "    \n",
    "    nb_epochs = 100\n",
    "    mini_batch_size = 10\n",
    "    \n",
    "    Train_error = []\n",
    "    Test_error = []\n",
    "    std_deviation = 0.0\n",
    "    train_error = 0.0\n",
    "    avg_nb_test_error = torch.tensor(())\n",
    "    \n",
    "    for i in range (0, len(Model)):\n",
    "        Model[i].lr_method(\"Adam\", 1.0e-3)\n",
    "        \n",
    "        train_model(Model[i], train_input_list[i], train_classes_list[i], nb_epochs, mini_batch_size)\n",
    "        \n",
    "        nb_train_errors = compute_nb_errors(Model[i], train_input_list[i], train_classes_list[i], mini_batch_size)\n",
    "        train_error += nb_train_errors / 10\n",
    "            \n",
    "        nb_test_errors = compute_nb_errors(Model[i], test_input_list[i], test_classes_list[i], mini_batch_size)\n",
    "        nb_test_errors = torch.tensor([nb_test_errors/10]).float()\n",
    "        #print('train error {:f}'.format(nb_train_errors))\n",
    "        #print('test error {:f}'.format(nb_test_errors.item()))\n",
    "        avg_nb_test_error = torch.cat((avg_nb_test_error, nb_test_errors), 0)\n",
    "        \n",
    "    Train_error.append(train_error / len(Model))\n",
    "    Test_error.append(avg_nb_test_error.mean().tolist())\n",
    "    std_deviation = avg_nb_test_error.std().tolist()\n",
    "    \n",
    "    return Train_error, Test_error, std_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.790000000000006]\n",
      "[37.77000045776367]\n",
      "19.75116539001465\n"
     ]
    }
   ],
   "source": [
    "Train_error_ReLU, Test_error_ReLU, std_deviation_ReLU = train_and_test_errors(Model_ReLU_List, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "\n",
    "print(Train_error_ReLU)\n",
    "print(Test_error_ReLU)\n",
    "print(std_deviation_ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9400000000000004]\n",
      "[6.2049994468688965]\n",
      "1.0927873849868774\n"
     ]
    }
   ],
   "source": [
    "Train_error_ELU, Test_error_ELU, std_deviation_ELU = train_and_test_errors(Model_ELU_List, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_ELU)\n",
    "print(Test_error_ELU)\n",
    "print(std_deviation_ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.795]\n",
      "[6.059999942779541]\n",
      "1.23048996925354\n"
     ]
    }
   ],
   "source": [
    "Train_error_Leaky_ReLU, Test_error_Leaky_ReLU, std_deviation_Leaky_ReLU = train_and_test_errors(Model_Leaky_ReLU_List, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_Leaky_ReLU)\n",
    "print(Test_error_Leaky_ReLU)\n",
    "print(std_deviation_Leaky_ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 47.000000\n",
      "test error 7.900000\n",
      "train error 43.000000\n",
      "test error 7.500000\n",
      "train error 89.000000\n",
      "test error 12.500000\n",
      "train error 40.000000\n",
      "test error 5.900000\n",
      "train error 38.000000\n",
      "test error 6.600000\n",
      "train error 59.000000\n",
      "test error 10.000000\n",
      "train error 141.000000\n",
      "test error 15.000000\n",
      "train error 74.000000\n",
      "test error 10.100000\n",
      "train error 32.000000\n",
      "test error 5.200000\n",
      "train error 61.000000\n",
      "test error 8.300000\n",
      "train error 42.000000\n",
      "test error 6.500000\n",
      "train error 52.000000\n",
      "test error 7.400000\n",
      "train error 27.000000\n",
      "test error 7.600000\n",
      "train error 103.000000\n",
      "test error 15.800000\n",
      "train error 45.000000\n",
      "test error 7.400000\n",
      "train error 354.000000\n",
      "test error 34.599998\n",
      "train error 462.000000\n",
      "test error 49.599998\n",
      "train error 29.000000\n",
      "test error 5.800000\n",
      "train error 48.000000\n",
      "test error 8.700000\n",
      "train error 67.000000\n",
      "test error 8.400000\n",
      "[9.265000000000002]\n",
      "[12.039999008178711]\n",
      "10.938507080078125\n"
     ]
    }
   ],
   "source": [
    "Train_error_Sigmoid, Test_error_Sigmoid, std_deviation_Sigmoid = train_and_test_errors(Model_Sigmoid_List, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_Sigmoid)\n",
    "print(Test_error_Sigmoid)\n",
    "print(std_deviation_Sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 25.000000\n",
      "test error 5.900000\n",
      "train error 25.000000\n",
      "test error 7.600000\n",
      "train error 30.000000\n",
      "test error 5.500000\n",
      "train error 13.000000\n",
      "test error 5.600000\n",
      "train error 21.000000\n",
      "test error 5.200000\n",
      "train error 11.000000\n",
      "test error 5.300000\n",
      "train error 36.000000\n",
      "test error 4.900000\n",
      "train error 31.000000\n",
      "test error 7.300000\n",
      "train error 24.000000\n",
      "test error 4.900000\n",
      "train error 22.000000\n",
      "test error 4.100000\n",
      "train error 24.000000\n",
      "test error 4.500000\n",
      "train error 29.000000\n",
      "test error 4.700000\n",
      "train error 22.000000\n",
      "test error 5.300000\n",
      "train error 37.000000\n",
      "test error 7.500000\n",
      "train error 27.000000\n",
      "test error 5.900000\n",
      "train error 34.000000\n",
      "test error 4.800000\n",
      "train error 19.000000\n",
      "test error 4.600000\n",
      "train error 13.000000\n",
      "test error 4.800000\n",
      "train error 15.000000\n",
      "test error 4.800000\n",
      "train error 25.000000\n",
      "test error 4.400000\n",
      "[2.415]\n",
      "[5.380000591278076]\n",
      "1.0175305604934692\n"
     ]
    }
   ],
   "source": [
    "Train_error_Tanh, Test_error_Tanh, std_deviation_Tanh = train_and_test_errors(Model_Tanh_List, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_Tanh)\n",
    "print(Test_error_Tanh)\n",
    "print(std_deviation_Tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_ReLU_List_CE = []\n",
    "Model_Tanh_List_CE = []\n",
    "Model_Sigmoid_List_CE = []\n",
    "Model_Leaky_ReLU_List_CE = []\n",
    "Model_ELU_List_CE = []\n",
    "\n",
    "for k in range (0, 20):\n",
    "    Model_ReLU_List_CE.append(Sequential([Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25,25), ReLU(), Linear(25,2), ReLU()], CrossEntropyLoss()))\n",
    "    Model_Tanh_List_CE.append(Sequential([Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,25), Tanh(), Linear(25,2), Tanh()], CrossEntropyLoss()))\n",
    "    Model_Sigmoid_List_CE.append(Sequential([Linear(2,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,25), Sigmoid(), Linear(25,2), Sigmoid()], CrossEntropyLoss()))\n",
    "    Model_Leaky_ReLU_List_CE.append(Sequential([Linear(2,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,25), Leaky_ReLU(), Linear(25,2), Leaky_ReLU()], CrossEntropyLoss()))\n",
    "    Model_ELU_List_CE.append(Sequential([Linear(2,25), ELU(), Linear(25,25), ELU(), Linear(25,25), ELU(), Linear(25,2), ELU()], CrossEntropyLoss()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_error_ReLU_CE, Test_error_ReLU_CE, std_deviation_ReLU_CE = train_and_test_errors(Model_ReLU_List_CE, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "\n",
    "print(Train_error_ReLU_CE)\n",
    "print(Test_error_ReLU_CE)\n",
    "print(std_deviation_ReLU_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_error_ELU_CE, Test_error_ELU_CE, std_deviation_ELU_CE = train_and_test_errors(Model_ELU_List_CE, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_ELU_CE)\n",
    "print(Test_error_ELU_CE)\n",
    "print(std_deviation_ELU_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_error_Tanh_CE, Test_error_Tanh_CE, std_deviation_Tanh_CE = train_and_test_errors(Model_Tanh_List_CE, train_input_list, train_classes_list, test_input_list, test_classes_list)\n",
    "print(Train_error_Tanh_CE)\n",
    "print(Test_error_Tanh_CE)\n",
    "print(std_deviation_Tanh_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 48.20% 482.000000/1000.000000\n",
      "test error 50.50% 505.000000/1000.000000\n",
      "test error 48.60% 486.000000/1000.000000\n",
      "test error 47.20% 472.000000/1000.000000\n",
      "test error 51.50% 515.000000/1000.000000\n",
      "test error 53.30% 533.000000/1000.000000\n",
      "test error 51.90% 519.000000/1000.000000\n",
      "test error 51.70% 517.000000/1000.000000\n",
      "test error 48.70% 487.000000/1000.000000\n",
      "test error 49.10% 491.000000/1000.000000\n",
      "test error 49.90% 499.000000/1000.000000\n",
      "Average test error 50.24% 502.4/1000\n"
     ]
    }
   ],
   "source": [
    "nb_train_errors = compute_nb_errors(model, train_input, train_classes, mini_batch_size)\n",
    "print('train error {:0.2f}% {:f}/{:f}'.format((100 * nb_train_errors) / train_input.size(0), nb_train_errors, train_classes.size(0)))\n",
    "\n",
    "L = get_tests(10)\n",
    "average_nb_test_error = 0\n",
    "for k in range (0, len(L)):\n",
    "    nb_test_errors = compute_nb_errors(model, L[k][0], L[k][1], mini_batch_size)\n",
    "    average_nb_test_error += nb_test_errors\n",
    "    print('test error {:0.2f}% {:f}/{:f}'.format((100 * nb_test_errors) / L[k][0].size(0), nb_test_errors, L[k][0].size(0)))\n",
    "print('Average test error {:0.2f}% {:0.1f}/{:d}'.format((100*average_nb_test_error/len(L)) / L[0][0].size(0), average_nb_test_error/len(L), L[0][0].size(0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
